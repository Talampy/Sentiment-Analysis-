{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22266    @_toni_b LA! beautiful people, great weather, the night life...I want! I want! wondering when I ...\n",
       "43718                                                                             @Angrylittlegirl attached \n",
       "70634                                                               @butterflybeacon Many Blessings to you! \n",
       "36752    @AlexDScott @Dojie  Sorrie  but i cant help it  assassinating is a hard job!! lol have a break h...\n",
       "59888                                                                            @Bellasaona Congratulation \n",
       "2808                                                                          mayfield psychiatric hospital.\n",
       "49077                                                                   @ashleigh92 aww baby  i love you xxx\n",
       "92181                @chrisdavidmills um, you must have a bootleg copy. mine starts with pseudo silk kimono \n",
       "975                                                                                      __cheer up emo kid.\n",
       "8145                                                                            #twitterchatsession  Love it\n",
       "66803                                              @britneyspears make a tour dvd please  happy mother's day\n",
       "25423                                                                @3baid congrats I wish they except you \n",
       "95734                             @CJSensei Unfortunately the road trip will have to happen only in my mind \n",
       "3094                                                                       - I want The Sims 3 soooo bad!!! \n",
       "23305                                                       @07thking Good Morning, K!  http://myloc.me/25p4\n",
       "89499    @clairelouise2 you might notice that I don't post about circ 'all day long', but only when neede...\n",
       "93446                                                                          @Bball4life Enjoy your nap.  \n",
       "33135         @alisonhogarth; I've completed it now so no need to worry. I'll try and get back into pro evo \n",
       "66909                                     @bnetwestival Oh no!!  I'm in Sydney for IMATS on the 12th!!      \n",
       "61144                                                          @bews 'Ere, does &quot;he&quot; have a name? \n",
       "79921                                     @CandyTX haha, sounds wonderful! Glad you are having a good time! \n",
       "85043    @chriscardell Just flicked C4 on, not loaded hills account yet though, most winners yesterday 12...\n",
       "71173                                                                 @brian_corrigan because you're a pool \n",
       "73447                                    @candra_ http://twitpic.com/6ccqn - that is hilarious and awesome. \n",
       "84809                                        @athoob that is attractive to me. 3ayal all dried up and torn? \n",
       "39557    @AnasaNaturals Thank u mama, I'm so happy I have u in my life - we're right here &amp; u've alre...\n",
       "29261      @acmorawski damai, like the condo. like damai because sue LIVES THERE?  btw, yes to tennis. &lt;3\n",
       "71875                    @Calcobrena Its blocked at work  I'll have to check it out at home..thanks  though!\n",
       "17997       ..And Hello to all new followers! I am loving this. Feel free to tweet me til its uncomfortable \n",
       "77230                                          @catcameron at least not from mine to yours.  i'm sorry love.\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "train_data = pd.read_csv(\"tweets.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "rand_indexs = np.random.randint(1,len(train_data),30).tolist()\n",
    "train_data[\"SentimentText\"][rand_indexs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {';-D', 'xD', 'XD', ':p', ';)', ';P', ':D', 'xd', ';d', ';p', ':d', ';D', 'x)', ':-D', ';-)'}\n",
      "Sad emoticons: {':/', ':(', ':|', \":'(\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tweets_text = train_data.SentimentText.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "    sorted(emos_count,reverse=True)\n",
    "\n",
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 128915 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'you',\n",
       " '?',\n",
       " 'a',\n",
       " 'it',\n",
       " 'i',\n",
       " ';',\n",
       " 'and',\n",
       " '&',\n",
       " '...',\n",
       " 'my',\n",
       " 'for',\n",
       " 'is',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'me',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'quot',\n",
       " \"'m\",\n",
       " 'so',\n",
       " ':',\n",
       " 'but',\n",
       " '#',\n",
       " 'do',\n",
       " 'was',\n",
       " 'be',\n",
       " '..',\n",
       " 'not',\n",
       " 'your',\n",
       " 'are',\n",
       " 'just',\n",
       " 'with',\n",
       " 'like',\n",
       " '-',\n",
       " 'at',\n",
       " '*',\n",
       " 'too',\n",
       " 'get',\n",
       " 'good',\n",
       " 'u']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train_data.SentimentText.str.cat())[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 128915 different words\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "mw = most_used_words(train_data.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "        if w in stopwords.words(\"english\"):\n",
    "            continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '/',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '3',\n",
       " '30',\n",
       " '30SECONDSTOMARS',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'AND',\n",
       " 'Ah',\n",
       " 'AlexAllTimeLow',\n",
       " 'All',\n",
       " 'Alyssa_Milano',\n",
       " 'And',\n",
       " 'Are',\n",
       " 'As',\n",
       " 'At',\n",
       " 'Aw',\n",
       " 'Awesome',\n",
       " 'Aww',\n",
       " 'Awww',\n",
       " 'Birthday',\n",
       " 'But',\n",
       " 'Ca',\n",
       " 'Can',\n",
       " 'Come',\n",
       " 'Congrats',\n",
       " 'Cool',\n",
       " 'D',\n",
       " 'DM',\n",
       " 'Damn',\n",
       " 'Day',\n",
       " 'Did',\n",
       " 'Do',\n",
       " 'Enjoy',\n",
       " 'FF',\n",
       " 'For',\n",
       " 'Friday',\n",
       " 'Get',\n",
       " 'Glad',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Got',\n",
       " 'Great',\n",
       " 'Haha',\n",
       " 'Happy',\n",
       " 'Have',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'Hope',\n",
       " 'How',\n",
       " 'I',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'If',\n",
       " 'Im',\n",
       " 'In',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Its',\n",
       " 'June',\n",
       " 'Just',\n",
       " 'LOL',\n",
       " 'LOVE',\n",
       " 'Let',\n",
       " 'Like',\n",
       " 'Lol',\n",
       " 'Love',\n",
       " 'ME',\n",
       " 'MY',\n",
       " 'Maybe',\n",
       " 'Me',\n",
       " 'Monday',\n",
       " 'Morning',\n",
       " 'My',\n",
       " 'NO',\n",
       " 'NOT',\n",
       " 'New',\n",
       " 'Nice',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'O',\n",
       " 'OMG',\n",
       " 'Oh',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'One',\n",
       " 'Or',\n",
       " 'Please',\n",
       " 'Poor',\n",
       " 'Really',\n",
       " 'S',\n",
       " 'SO',\n",
       " 'Saturday',\n",
       " 'See',\n",
       " 'She',\n",
       " 'So',\n",
       " 'Sorry',\n",
       " 'Sounds',\n",
       " 'Still',\n",
       " 'Sunday',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'TO',\n",
       " 'TV',\n",
       " 'Thank',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'They',\n",
       " 'This',\n",
       " 'To',\n",
       " 'Too',\n",
       " 'Twitter',\n",
       " 'U',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'Very',\n",
       " 'Was',\n",
       " 'We',\n",
       " 'Welcome',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Who',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'Wish',\n",
       " 'Would',\n",
       " 'Wow',\n",
       " 'XD',\n",
       " 'YAY',\n",
       " 'YOU',\n",
       " 'Yay',\n",
       " 'Yeah',\n",
       " 'Yep',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'Your',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'account',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'aint',\n",
       " 'album',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andyclemmensen',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'aplusk',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'ashleytisdale',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'at',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'b',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'babygirlparis',\n",
       " 'back',\n",
       " 'backstreetboys',\n",
       " 'bad',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'blog',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'both',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'bradiewebbstack',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'btw',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'chat',\n",
       " 'check',\n",
       " 'chocolate',\n",
       " 'class',\n",
       " 'close',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'computer',\n",
       " 'concert',\n",
       " 'congrats',\n",
       " 'cool',\n",
       " 'cos',\n",
       " 'could',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'def',\n",
       " 'definitely',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'different',\n",
       " 'dinner',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'down',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'enjoying',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'excited',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fail',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'fell',\n",
       " 'few',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'follow',\n",
       " 'followers',\n",
       " 'followfriday',\n",
       " 'following',\n",
       " 'food',\n",
       " 'for',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'from',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'game',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gon',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hate',\n",
       " 'have',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'hehe',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'his',\n",
       " 'hit',\n",
       " 'hmm',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'http',\n",
       " 'hug',\n",
       " 'huge',\n",
       " 'hugs',\n",
       " 'huh',\n",
       " 'hun',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'i',\n",
       " 'iPhone',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'idk',\n",
       " 'if',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'in',\n",
       " 'inaperfectworld',\n",
       " 'indeed',\n",
       " 'info',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'iremember',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'ive',\n",
       " 'jealous',\n",
       " 'job',\n",
       " 'join',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kid',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'lady',\n",
       " 'lame',\n",
       " 'laptop',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'life',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'lil',\n",
       " 'line',\n",
       " 'link',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lmao',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loves',\n",
       " 'lt',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'mad',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'mate',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mention',\n",
       " 'message',\n",
       " 'met',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morning',\n",
       " 'most',\n",
       " 'move',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'moving',\n",
       " 'much',\n",
       " 'mum',\n",
       " 'music',\n",
       " 'musicmonday',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'myweakness',\n",
       " 'n',\n",
       " \"n't\",\n",
       " 'na',\n",
       " 'name',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'no',\n",
       " 'nope',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'number',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'only',\n",
       " 'open',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'own',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'pics',\n",
       " 'picture',\n",
       " 'pictures',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'poor',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'ppl',\n",
       " 'pretty',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'proud',\n",
       " 'put',\n",
       " 'question',\n",
       " 'quite',\n",
       " 'quot',\n",
       " 'r',\n",
       " 'rain',\n",
       " 'raining',\n",
       " 'rather',\n",
       " 're',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'red',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'rest',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'rock',\n",
       " 'room',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'sadly',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'same',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'send',\n",
       " 'sent',\n",
       " 'seriously',\n",
       " 'set',\n",
       " 'shall',\n",
       " 'share',\n",
       " 'she',\n",
       " 'shit',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'should',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'shut',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'sigh',\n",
       " 'sign',\n",
       " 'silly',\n",
       " 'since',\n",
       " 'sister',\n",
       " 'site',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smile',\n",
       " 'so',\n",
       " 'some',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'song',\n",
       " 'songs',\n",
       " 'soo',\n",
       " 'soon',\n",
       " 'sooo',\n",
       " 'soooo',\n",
       " 'sorry',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'special',\n",
       " 'squarespace',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'stay',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'stuck',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'such',\n",
       " 'suck',\n",
       " 'sucks',\n",
       " 'summer',\n",
       " 'sun',\n",
       " 'sunday',\n",
       " 'sunny',\n",
       " 'super',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'sweet',\n",
       " 'ta',\n",
       " 'take',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'text',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'that',\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'this',\n",
       " 'tho',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thx',\n",
       " 'tickets',\n",
       " 'til',\n",
       " 'till',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'to',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'tonight',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'totally',\n",
       " 'tour',\n",
       " 'town',\n",
       " 'train',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'tweet',\n",
       " 'tweeting',\n",
       " 'tweets',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'u',\n",
       " 'ugh',\n",
       " 'under',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'until',\n",
       " 'up',\n",
       " 'update',\n",
       " 'updates',\n",
       " 'upset',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'version',\n",
       " 'very',\n",
       " 'via',\n",
       " 'video',\n",
       " 'vip',\n",
       " 'visit',\n",
       " 'w/',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'watch',\n",
       " 'watched',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'we',\n",
       " 'wear',\n",
       " 'weather',\n",
       " 'website',\n",
       " 'wedding',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'why',\n",
       " 'will',\n",
       " 'win',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'without',\n",
       " 'wo',\n",
       " 'woke',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wow',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'wrong',\n",
       " 'x',\n",
       " 'xD',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'ya',\n",
       " 'yay',\n",
       " 'yea',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yep',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'yo',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'youtube',\n",
       " 'yup',\n",
       " '»']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(most_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 128915 different words\n",
      "There is 128915 different words\n"
     ]
    }
   ],
   "source": [
    "most_used_words(train_data.SentimentText.str.cat())[:100]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "mw = most_used_words(train_data.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "        if w in stopwords.words(\"english\"):\n",
    "            continue\n",
    "        else:\n",
    "            most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X, y=None):\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "            \n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            X = X.str.replace(\"#\", \"\")\n",
    "            X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "            # Removing HTML garbage\n",
    "            X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "            # Removing links\n",
    "            X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "            # replace repeated letters with only two occurences\n",
    "            # heeeelllloooo => heelloo\n",
    "            X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "            # mark emoticons as happy or sad\n",
    "            X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "            X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "            X = X.str.lower()\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Sentiments = train_data['Sentiment']\n",
    "tweets = train_data['SentimentText']\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "('vectorizer', vectorizer),\n",
    "])\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, Sentiments, test_size=0.3)\n",
    "learning_data = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SentimentText', 'Sentiment')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = train_data.columns.values[2]\n",
    "sentiment = train_data.columns.values[1]\n",
    "tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negetiveemoji ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negetiveemoji ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "with open('contractions.json', 'r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tweet = tweet.lower()                                             \n",
    "    tweet = re.sub('@[^\\s]+', '', tweet)                              \n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   \n",
    "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           \n",
    "    tweet = re.sub('&quot;',\" \", tweet)                               \n",
    "    tweet = emoji(tweet)                                              \n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   \n",
    "    for word in tweet.split():\n",
    "        if word.lower() in contractions:\n",
    "            tweet = tweet.replace(word, contractions[word.lower()])  \n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       \n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         \n",
    "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['processed_tweet'] = np.vectorize(process_tweet)(train_data[tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99989x455478 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2112514 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))    \n",
    "final_vectorized_data = count_vectorizer.fit_transform(train_data['processed_tweet'])  \n",
    "final_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, train_data[sentiment],\n",
    "                                                    test_size=0.2, random_state=71)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    " \n",
    "model_naive = MultinomialNB().fit(X_train, y_train) \n",
    "predicted_naive = model_naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Naive-bayes Algorothm:  0.7623762376237624\n"
     ]
    }
   ],
   "source": [
    "# Findig the Accurracy of the prediction using the Naive Bayes algorithm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_naive = accuracy_score(predicted_naive, y_test)\n",
    "print(\"Accuracy using Naive-bayes Algorothm: \",score_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73      8732\n",
      "           1       0.79      0.78      0.79     11266\n",
      "\n",
      "    accuracy                           0.76     19998\n",
      "   macro avg       0.76      0.76      0.76     19998\n",
      "weighted avg       0.76      0.76      0.76     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating the precision, recall and accurracy - Naive Bayes\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Accurracy\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    " \n",
    "model_lr = LogisticRegression().fit(X_train, y_train) \n",
    "predicted_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Algorothm:  0.7758775877587759\n"
     ]
    }
   ],
   "source": [
    "# Findig the Accurracy of the prediction using  Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "score_lr = accuracy_score(predicted_lr, y_test)\n",
    "print(\"Accuracy of Logistic Regression Algorothm: \",score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    " \n",
    "model_lr= LogisticRegression().fit(X_train, y_train) \n",
    "predicted_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.71      0.73      8732\n",
      "           1       0.79      0.83      0.81     11266\n",
      "\n",
      "    accuracy                           0.78     19998\n",
      "   macro avg       0.77      0.77      0.77     19998\n",
      "weighted avg       0.78      0.78      0.77     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating the precision, recall and accurracy - Logistic Regression\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    " \n",
    "model_svm = SVC().fit(X_train, y_train) \n",
    "predicted_svm = model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72      8732\n",
      "           1       0.77      0.84      0.80     11266\n",
      "\n",
      "    accuracy                           0.77     19998\n",
      "   macro avg       0.77      0.76      0.76     19998\n",
      "weighted avg       0.77      0.77      0.77     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating the precision, recall and accurracy - Logistic Regression\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM:  0.7681768176817682\n"
     ]
    }
   ],
   "source": [
    "# Findig the Accurracy of the prediction using  SVM\n",
    "from sklearn.svm import SVC\n",
    "score_svm = accuracy_score(predicted_svm, y_test)\n",
    "print(\"Accuracy of SVM: \",score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
